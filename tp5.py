# -*- coding: utf-8 -*-
"""TP5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1psTB1H1Y7YLGq4B_HB8n_mZkQOjzsBKA
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import export_graphviz
import graphviz
import missingno as msno
from scipy.stats import ttest_rel

"""# Pré-processamento:

Carregando dataset:
"""

import pandas as pd
df = pd.read_csv('healthcare-dataset-stroke-data.csv')
df

"""Verificando dados ausentes:"""

df.isna().sum()

msno.bar(df)

"""Tratando dados ausentes da coluna 'bmi' utilizando a média."""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
imputer.fit_transform
df[['bmi']] = imputer.fit_transform(df[['bmi']])

"""Dataset depois de tratar valores ausentes:"""

df.isna().sum()

"""Codificando as variaveis utilizando o labelEncoder:"""

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
df = df.apply(label_encoder.fit_transform)
df

"""Gerando matriz de correlação:

*Threshold definido como 0.6*
"""

from matplotlib import pyplot as plt
import seaborn as sn
corr_matrix = df.corr().abs()
filtered_corr_df = corr_matrix
plt.figure(figsize=(10,10))
sn.heatmap(filtered_corr_df, annot=True, cmap="Reds")

"""Verificando o desbalanceamento do dataset:"""

np.unique(df['stroke'], return_counts=True)
sns.countplot(x = df['stroke'])

"""Dividindo o dataset em atributos de entrada *X* e atributos de classe *y*"""

X = df.drop('stroke', axis=1)
y = (df['stroke'])

"""Realizando o train_test_split:"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3)
X_train_ind, X_test_ind, y_train_ind, y_test_ind = train_test_split(X, y,stratify=y,test_size=0.3)

"""Teste sem k-fold"""

from imblearn.combine import SMOTETomek
from sklearn.metrics import classification_report, precision_recall_fscore_support
from sklearn.model_selection import GridSearchCV, KFold
from imblearn.combine import SMOTETomek
from scipy import stats
# X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3)
os = SMOTETomek(sampling_strategy=0.6)
X_train_ns_ind,y_train_ns_ind=os.fit_resample(X_train_ind,y_train_ind)


# Definir os hiperparâmetros a serem otimizados para cada modelo
rf_params = { 'criterion':  ['gini', 'entropy'],
    'max_depth':  [None, 2, 4, 6, 8, 10],
    'max_features': [None, 'sqrt', 'log2', 0.2, 0.4, 0.6, 0.8],}
dt_params = { 'criterion':  ['gini', 'entropy'],
    'max_depth':  [None, 2, 4, 6, 8, 10],
    'max_features': [None, 'sqrt', 'log2', 0.2, 0.4, 0.6, 0.8],}
# rf_params = { 'criterion':  ['gini', 'entropy']}
# dt_params = { 'criterion':  ['gini', 'entropy'],}

# Criar instâncias dos modelos com os hiperparâmetros padrão
rf_model = RandomForestClassifier()
dt_model = DecisionTreeClassifier()

# Aplicar o GridSearch para cada modelo
rf_grid_search = GridSearchCV(rf_model, rf_params, cv=5)
rf_grid_search.fit(X_train_ns_ind, y_train_ns_ind)

dt_grid_search = GridSearchCV(dt_model, dt_params, cv=5)
dt_grid_search.fit(X_train_ns_ind, y_train_ns_ind)

# Obter os melhores modelos encontrados pelo GridSearch
best_rf_model = rf_grid_search.best_estimator_
best_dt_model = dt_grid_search.best_estimator_

# Fazer previsões com os melhores modelos
y_pred_test_rf = best_rf_model.predict(X_test_ind)
y_pred_test_dt = best_dt_model.predict(X_test_ind)

# Imprimir os relatórios de classificação
print("Random Forest:")
print(classification_report(y_test, y_pred_test_rf))

print("Decision Tree:")
print(classification_report(y_test, y_pred_test_dt))

# Imprimir os melhores parâmetros encontrados para o Random Forest
print("Melhores parâmetros Random Forest:")
print(rf_grid_search.best_params_)

# Imprimir os melhores parâmetros encontrados para a Decision Tree
print("Melhores parâmetros Decision Tree:")
print(dt_grid_search.best_params_)

import matplotlib.pyplot as plt

# Crie um DataFrame com os dados balanceados
balanced_df = pd.DataFrame(X_train_ns_ind, columns=X_train.columns)
balanced_df['target'] = y_train_ns_ind

# Conte a quantidade de amostras em cada classe
class_counts = balanced_df['target'].value_counts()

# Obtenha as classes e as contagens correspondentes
classes = ['Classe Positiva', 'Classe Negativa']
counts = [class_counts[1], class_counts[0]]  # Considerando que a classe positiva é representada pelo valor 1

# Crie um gráfico de barras para mostrar o balanceamento
plt.figure(figsize=(8, 6))
plt.bar(classes, counts)
plt.xlabel('Classes')
plt.ylabel('Contagem')
plt.title('Distribuição do Dataset após o Balanceamento')
plt.show()

"""# Treinando modelos e avaliando o desempenho:

Função para calcular o intervalo de confiança:
"""

from sklearn.metrics import precision_recall_fscore_support
def interval_confidence(values):
    return stats.t.interval(confidence=0.95, df=len(values)-1, loc=np.mean(values), scale=stats.sem(values))

"""Função que realiza a validação cruzada K-fold e retorna o melhor modelo."""

from sklearn.metrics import classification_report, precision_recall_fscore_support
from sklearn.model_selection import GridSearchCV, KFold
from imblearn.combine import SMOTETomek
from scipy import stats

def cross(X_train, y_train, k, model):
    f1_score_superior = []
    f1_score_inferior = []
    recall_superior = []
    recall_inferior = []
    precisao_superior = []
    precisao_inferior = []
    precision_valid = [0, 0]
    recall_valid = [0, 0]
    fscore_valid = [0, 0]
    skf = KFold(n_splits=k, shuffle=True)  # Divisão em k folds estratificados
    scores = []  # Lista para armazenar as métricas de cada fold
    params = {
        'criterion':  ['gini', 'entropy'],
        'max_depth':  [None, 2, 4, 6, 8, 10],
        'max_features': [None, 'sqrt', 'log2', 0.2, 0.4, 0.6, 0.8],
    }

    best_model = None
    best_score = 0  # Inicialização do melhor score encontrado

    for train_index, val_index in skf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Dados de treino e validação do fold
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Rótulos de treino e validação do fold

        # balanceando nos dados de treino do fold
        os = SMOTETomek(sampling_strategy=0.6)
        X_train_resampled, y_train_resampled = os.fit_resample(X_train_fold, y_train_fold)

        # GridSearch no fold
        grid_search = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', cv=3)
        grid_search.fit(X_train_resampled, y_train_resampled)

        # Melhores hiperparâmetros encontrados
        best_params = grid_search.best_params_

        # Treinamento do modelo com os melhores hiperparâmetros
        model.set_params(**best_params)
        model.fit(X_train_resampled, y_train_resampled)

        # Avaliação do modelo no conjunto de validação do fold
        y_pred = model.predict(X_val_fold)
        score = model.score(X_val_fold, y_val_fold)
        scores.append(score)
        # Positivo e negativo.
        precision, recall, fscore, support = precision_recall_fscore_support(y_val_fold, y_pred, average=None)
        precision_valid = np.add(precision_valid, precision)
        recall_valid = np.add(recall_valid, recall)
        fscore_valid = np.add(fscore_valid, fscore)
        f1_score_superior.append(fscore[1])  # Corrigido
        f1_score_inferior.append(fscore[0])  # Corrigido

        recall_superior.append(recall[1])  # Corrigido
        recall_inferior.append(recall[0])  # Corrigido

        precisao_superior.append(precision[1])  # Corrigido
        precisao_inferior.append(precision[0])  # Corrigido

        # Verificar se o modelo atual é melhor que o melhor modelo anterior
        if score > best_score:
            best_score = score
            best_model = model

        # Relatório de classificação do fold
        print(f"Fold {len(scores)}:")
        print("Melhores parametros para o fold:", len(scores))
        print(best_params)
        print(classification_report(y_val_fold, y_pred))
        print("--------------------------------------------")

    # Plotar gráfico de barras para o F1 score superior e inferior
    plt.figure(figsize=(8, 6))
    sns.barplot(x=['Superior', 'Inferior'], y=[np.mean(f1_score_superior), np.mean(f1_score_inferior)])
    plt.title('F1 Score Médio')
    plt.xlabel('Classe')
    plt.ylabel('F1 Score')
    plt.show()

    # Plotar gráfico de barras para o recall superior e inferior
    plt.figure(figsize=(8, 6))
    sns.barplot(x=['Superior', 'Inferior'], y=[np.mean(recall_superior), np.mean(recall_inferior)])
    plt.title('Recall Médio')
    plt.xlabel('Classe')
    plt.ylabel('Recall')
    plt.show()

    # Plotar gráfico de barras para a precisão superior e inferior
    plt.figure(figsize=(8, 6))
    sns.barplot(x=['Superior', 'Inferior'], y=[np.mean(precisao_superior), np.mean(precisao_inferior)])
    plt.title('Precisão Média')
    plt.xlabel('Classe')
    plt.ylabel('Precisão')
    plt.show()

    # Exibição das médias e intervalos de confiança
    print("Intervalo de confiança do Fscore superior: ", interval_confidence(f1_score_superior))
    print("Intervalo de confiança do Fscore inferior: ", interval_confidence(f1_score_inferior))
    print("Intervalo de confiança do Recall superior: ", interval_confidence(recall_superior))
    print("Intervalo de confiança do Recall inferior: ", interval_confidence(recall_inferior))
    print("Intervalo de confiança da Precisao superior: ", interval_confidence(precisao_superior))
    print("Intervalo de confiança da Precisao inferior: ", interval_confidence(precisao_inferior))
    print("----------------------------------------------------------------------------------------")
    print("F1 score médio na validação das classes Inferior e Superior: ", np.round(fscore_valid / k, 2))
    print("Recall médio na validação das classes Inferior e Superior: ", np.round(recall_valid / k, 2))
    print("Precisão média na validação das classes Inferior e Superior: ", np.round(precision_valid / k, 2))

    return best_model

"""Instanciando modelos:"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
rf = RandomForestClassifier()
dt = DecisionTreeClassifier()

"""Realizando a validação cruzada e printando o classification report do melhor modelo encontrado."""

from sklearn.metrics import accuracy_score
print("RANDOM FOREST:")
best_model_rf = cross(X_train, y_train, 5, rf)
y_pred_test_rf = best_model_rf.predict(X_test)
print("Melhor   random forest: ------------------------------")
print(classification_report(y_test, y_pred_test_rf))

print("DECISION TREE:")
best_model_dt = cross(X_train, y_train, 5, rf)
y_pred_test_dt = best_model_dt.predict(X_test)
print("Melhor modelo Decision Tree: ------------------------------")
print(classification_report(y_test, y_pred_test_dt))

# Calculate t-test
metric_rf = accuracy_score(y_test, y_pred_test_rf)  # Replace with your desired performance metric
metric_dt = accuracy_score(y_test, y_pred_test_dt)  # Replace with your desired performance metric
n_rf = len(y_pred_test_rf)
n_dt = len(y_pred_test_dt)
std_rf = np.std([metric_rf] * n_rf)
std_dt = np.std([metric_dt] * n_dt)

t_value = (metric_rf - metric_dt) / np.sqrt((std_rf**2 / n_rf) + (std_dt**2 / n_dt))
df = n_rf + n_dt - 2  # Degrees of freedom
alpha = 0.05  # Significance level
from scipy.stats import t
# Calculate critical value for two-tailed test
critical_value = t.ppf(1 - alpha/2, df)
print("T-value:", t_value)
print("Critical value:", critical_value)
print("Precision - Random Forest:", metric_rf)
print("Precision - Decision Tree:", metric_dt)
# Compare t-value with critical value
if abs(t_value) > critical_value:
    print("There is a statistically significant difference between the models.")
else:
    print("There is no statistically significant difference between the models.")

"""Comparando os dois modelos em relação a o seu ROC score."""

from sklearn import metrics
fpr, tpr, thresh = metrics.roc_curve(y_test, y_pred_test_rf)
auc = metrics.roc_auc_score(y_test, y_pred_test_rf)
plt.plot(fpr,tpr,label="RF, auc="+str('{0:.4f}'.format(auc)))

fpr, tpr, thresh = metrics.roc_curve(y_test, y_pred_test_dt)
auc = metrics.roc_auc_score(y_test, y_pred_test_dt)
plt.plot(fpr,tpr,label="DT, auc="+str('{0:.4f}'.format(auc)))
plt.title("Performance dos modelos")
plt.xlabel("1-Specificity(Taxa de falso positivo)")
plt.ylabel("Sensitivity(True Positive Rate)")
plt.legend(loc=0)
plt.show()

"""F1-Score"""

f1_rf = metrics.f1_score(y_test, y_pred_test_rf)
f1_dt = metrics.f1_score(y_test, y_pred_test_dt)

models = ['Random Forest', 'Decision Tree']
f1_scores = [f1_rf, f1_dt]
colors = ['blue', 'orange']  # Cores para as barras

plt.bar(models, f1_scores, color=colors)
plt.title("Comparação de F1 Score")
plt.xlabel("Modelos")
plt.ylabel("F1 Score")
plt.show()

"""Precisão:"""

precision_rf = metrics.precision_score(y_test, y_pred_test_rf)
precision_dt = metrics.precision_score(y_test, y_pred_test_dt)

models = ['Random Forest', 'Decision Tree']
precision_scores = [precision_rf, precision_dt]
colors = ['blue', 'orange']

plt.bar(models, precision_scores, color=colors)
plt.title("Comparação de Precisão")
plt.xlabel("Modelos")
plt.ylabel("Precisão")
plt.show()

"""Recall:"""

# Recall
recall_rf = metrics.recall_score(y_test, y_pred_test_rf)
recall_dt = metrics.recall_score(y_test, y_pred_test_dt)

models = ['Random Forest', 'Decision Tree']
recall_scores = [recall_rf, recall_dt]
colors = ['blue', 'orange']

plt.bar(models, recall_scores, color=colors)
plt.title("Comparação de Recall")
plt.xlabel("Modelos")
plt.ylabel("Recall")
plt.show()